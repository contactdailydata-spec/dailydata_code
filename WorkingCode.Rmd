---
title: "WorkingCode"
output: html_document
date: "2025-09-09"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Libraries
```{r}
#install.packages("ggplot2")
#install.packages("tidyr")
#install.packages("dplyr")
library(ggplot2)
library(tidyr)
library(dplyr)

#install.packages("showtext")
library(showtext)

#install.packages("scales")
library(scales)
```


# Unemployment by age over years
## Read file
```{r}
Unemployment_Age_Year<-read.csv("Unemployment_Age_Year.csv")
```

## Clean dataset
```{r}
# Changing numbers from character with a % sign to numeric 
Unemployment_Age_Year$X20.24.Years <- as.numeric(gsub("%", "", Unemployment_Age_Year$X20.24.Years))
Unemployment_Age_Year$X25.34.Years <- as.numeric(gsub("%", "", Unemployment_Age_Year$X25.34.Years))
Unemployment_Age_Year$X35.44.Years <- as.numeric(gsub("%", "", Unemployment_Age_Year$X35.44.Years))
Unemployment_Age_Year$X45.54.Years <- as.numeric(gsub("%", "", Unemployment_Age_Year$X45.54.Years))
Unemployment_Age_Year$X55..Years <- as.numeric(gsub("%", "", Unemployment_Age_Year$X55..Years))

# changing column names so they are cleaner
colnames(Unemployment_Age_Year)<-c("Year","20-24","25-34","35-44","45-55","55+")

# Long format
Unemployment_Age_Year_long <- Unemployment_Age_Year %>%
  pivot_longer(cols = -Year, names_to = "AgeGroup", values_to = "Value")
```

## Graph
```{r}

font_add_google("Lato", "lato")  # load Lato font with alias "lato"
showtext_auto()                      # Enable showtext for future plots

myplot<-ggplot(Unemployment_Age_Year_long, aes(x = Year, y = Value, color = AgeGroup)) +
  geom_line(size = 1) +
  # Add vertical lines for 2008 and 2020
  geom_vline(xintercept = 2008, linetype = "dashed", color = "gray40", size = 0.8) +
  geom_vline(xintercept = 2020, linetype = "dashed", color = "gray40", size = 0.8) +
  # Add text annotations for the events
  annotate("text", x = 2008, y = max(Unemployment_Age_Year_long$Value) * 0.95, 
           label = "2008 Recession", angle = 90, hjust = .75, vjust = -0.5, size = 4, color = "gray20") +
  annotate("text", x = 2020, y = max(Unemployment_Age_Year_long$Value) * 0.95, 
           label = "COVID-19", angle = 90, hjust=.5, vjust = -0.5, size = 4, color = "gray20") +
  labs(
    title ="",
    x = "Year",
    y = "Unemployment Rate (%)",
    color = "Age Group"
  ) +
  theme_classic(base_family = "lato") +  # use the Google font alias here
  theme(
    plot.title = element_text(size = 18, face = "bold", hjust = 0.5),
    axis.title = element_text(size = 14),
    axis.text = element_text(size = 12),
    axis.text.x = element_text(angle = 45, hjust = 1),
    legend.title = element_text(size = 13),
    legend.text = element_text(size = 11),
    legend.position = "bottom"
  )+
    scale_color_manual(values = c(
    "20-24"  = "#264653",  # dark blue-green
    "25-34" = "#2a9d8f",  # medium teal
    "35-44" = "#e9c46a",  # warm yellow
    "45-55" = "#f4a261",  # soft orange
    "55+"   = "#e76f51"   # muted red-orange
  ))

```

# Approximate number of Refugees hosted by country as of end of 2024
```{r}
CountryRefugees<-read.csv("CountryRefugees.csv") # add data
colnames(CountryRefugees)<-c("Country", "Refugees") # change column names
CountryRefugees$Refugees <- as.numeric(gsub(",", "", CountiesRefugees$Refugees)) # remove comma and make numeric
CountryRefugees$Country <- factor(CountryRefugees$Country, levels = CountryRefugees$Country[order(CountryRefugees$Refugees)]) # put data in decending order

# make plot
myplot<-ggplot(CountryRefugees, aes(x = Country, y = Refugees, fill = Refugees)) +
  geom_bar(stat = "identity") +
  geom_text(aes(label = scales::comma(Refugees)), 
            hjust = -0.1, size = 3.5, color = "black") +
  coord_flip() +
  labs(title = NULL, x = "Host Country", y = "Number of Refugees") +
  theme_minimal(base_size = 14) +
  theme(
    axis.title = element_text(face = "bold"),
    axis.text.y = element_text(size = 12),
    axis.text.x = element_text(size = 10),
    plot.margin = margin(10, 20, 10, 10),
    legend.position = "none"  # hide legend if you want
  ) +
  scale_y_continuous(labels = scales::comma, expand = expansion(mult = c(0, 0.1))) +
  scale_fill_gradient(low = "#FFC107", high = "#D84315")  # warm yellow to deep red

```

# How world chess championship games have ended
```{r}

data <- data.frame(
  Result = rep(c("Checkmate", "Resignation", "Draw"), each = 2),
  Player = rep(c("Challenger", "Defender"), times = 3),
  Count = c(1, 1, 13, 18, 8, 10)
)

ggplot(data, aes(x = Result, y = Count, fill = Player)) +
  geom_bar(
    stat = "identity",
    position = position_dodge(width = 0.7),
    width = 0.6,   # thinner bars
    color = "grey30",
    size = 0.25
  ) +
  scale_fill_manual(
    values = c(
      "Challenger" = "#E69F00",  # orange
      "Defender"   = "#56B4E9"   # blue
    )
  ) +
  labs(
    title = "",
    x = "Result",
    y = "Number of Games",
    fill = "Player"
  ) +
  theme_minimal(base_size = 8) +
  theme(
    plot.title = element_text(size = 18, face = "bold", hjust = 0.5),
    axis.title = element_text(size = 14),
    axis.text = element_text(size = 12),
    legend.title = element_text(size = 13),
    legend.text = element_text(size = 12),
    panel.grid.major.x = element_blank(),
    panel.grid.minor = element_blank(),
    panel.grid.major.y = element_line(color = "grey85")
  )



plot_pie_edge_labels_pct <- function(df, title) {
  df <- df %>%
    arrange(desc(Result)) %>%
    mutate(
      fraction = Count / sum(Count),
      percent = paste0(round(fraction * 100), "%"),
      ymax = cumsum(fraction),
      ymin = c(0, head(ymax, n = -1)),
      label_position = (ymax + ymin) / 2
    )
  
  ggplot(df, aes(x = 1, y = fraction, fill = Result)) +
    geom_bar(stat = "identity", width = 1, color = "white", size = 0.8) +
    coord_polar(theta = "y") +
    geom_segment(aes(x = 1.5, xend = 1.7, y = label_position, yend = label_position),
                 color = "black", size = 0.6) +
    geom_text(aes(x = 2, y = label_position, label = percent),  # Show percentage labels
              hjust = 0, size = 4, fontface = "bold") +
    labs(title = title) +
    theme_void() +
    scale_fill_grey(start = 0.3, end = 0.8) +
    xlim(0.5, 2) +
    theme(
      plot.title = element_text(hjust = 0.5, size = 16, face = "bold"),
      legend.position = "bottom",
      legend.title = element_blank(),
      legend.text = element_text(size = 12)
    )
}


challenger_data <- filter(data, Player == "Challenger")
plot_pie_edge_labels_pct(challenger_data, "Challenger's Results")

defender_data <- filter(data, Player == "Defender")
plot_pie_edge_labels_pct(defender_data, "Defender's Results")


```




# Chess checkmate heatmap
```{r}
#install.packages("patchwork")
library(patchwork)
# -----------------------------
# Dataset 1: Number of Checkmate Possibilities
# -----------------------------
chess_data1 <- tribble(
  ~rank, ~a, ~b, ~c, ~d, ~e, ~f, ~g, ~h,
  8, 10000, 10000, 10000, 10000, 10000, 10000, 10000, 10000,
  7, 10000, 20000, 20000, 20000, 20000, 20000, 20000, 10000,
  6, 10000, 20000, 30000, 30000, 30000, 30000, 20000, 10000,
  5, 8333, 16667, 25000, 33337, 33333, 25000, 16667, 8333,
  4, 8333, 16667, 25000, 33333, 33333, 25000, 16667, 8333,
  3, 6666, 13334, 20000, 20000, 20000, 20000, 13334, 6666,
  2, 6666, 13334, 13334, 13334, 13334, 13334, 13334, 6666,
  1, 6666, 6666, 6666, 6666, 6666, 6666, 6666, 6666
)

long_data1 <- chess_data1 %>%
  pivot_longer(cols = a:h, names_to = "file", values_to = "value") %>%
  mutate(file = factor(file, levels = c("a","b","c","d","e","f","g","h")))

p1<-ggplot(long_data1, aes(x = file, y = rank, fill = value)) +
  geom_tile(color = "grey30", size = 0.3) +
  scale_fill_gradient(low = "#fee5d9", high = "#de2d26", guide = "none") +
  scale_y_continuous(trans = "reverse", breaks = 1:8) +
  labs(title = "Heatmap of All Possible Checkmate Positions") +
  theme_void() +
  theme(
    plot.title = element_text(hjust = 0.5, size = 9, face = "bold")
  )

# -----------------------------
# Dataset 2: Actual Most Common Checkmate Positions
# -----------------------------
chess_data2 <- tribble(
  ~rank, ~a, ~b, ~c, ~d, ~e, ~f, ~g, ~h,
  8, 1.6, 0.9, 1.2, 1.5, 3.7, 2.7, 6.2, 3.1,
  7, 0.8, 1.1, 1.2, 1.5, 1.8, 1.5, 2.3, 1.0,
  6, 0.6, 0.9, 1.2, 1.5, 1.8, 1.4, 2.0, 0.9,
  5, 0.5, 0.8, 1.2, 1.5, 1.7, 1.3, 1.9, 0.8,
  4, 0.5, 0.7, 1.1, 1.4, 1.5, 1.1, 1.4, 0.7,
  3, 0.5, 0.7, 1.1, 1.4, 1.5, 1.1, 1.4, 0.7,
  2, 0.6, 0.9, 1.2, 1.4, 1.5, 1.1, 1.4, 0.8,
  1, 1.4, 1.1, 1.6, 2.0, 3.6, 2.6, 5.7, 3.9
)

long_data2 <- chess_data2 %>%
  pivot_longer(cols = a:h, names_to = "file", values_to = "value") %>%
  mutate(file = factor(file, levels = c("a","b","c","d","e","f","g","h")))

p2<-ggplot(long_data2, aes(x = file, y = rank, fill = value)) +
  geom_tile(color = "grey30", size = 0.3) +
  scale_fill_gradient(low = "#fff7f3", high = "#de2d26", guide = "none") +
  scale_y_continuous(trans = "reverse", breaks = 1:8) +
  labs(title = "Heatmap of Actual Most Common Checkmate Positions") +
  theme_void() +
  theme(
    plot.title = element_text(hjust = 0.5, size = 9, face = "bold")
  )

p1+p2
```


# Coffee trade across continents Sankey diagram
```{r}
# Load libraries
#install.packages("igraph")
library(igraph)
#install.packages("ggraph")
library(ggraph)
#install.packages("tidyverse")
library(tidyverse)

# Create the data frame using data from the UN Comtrade 2023
coffee <- tibble(
  exporter = c("Brazil", "Brazil", "Brazil", "Vietnam", "Vietnam",
               "Colombia", "Indonesia", "Honduras", "India", "Uganda", "Ethiopia"),
  importer = c("US", "Germany", "Italy", "US", "Japan",
               "US", "Japan", "US", "Germany", "Germany", "Germany"),
  value = c(1.47, 0.20, 0.10, 0.35, 0.43,
            0.77, 0.34, 0.44, 0.23, 0.26, 0.25) # example in million tonnes
)

coffee <- coffee %>%
  mutate(value_mkg = value * 1000)  # million tonnes â†’ million kg

# Load ggalluvial (assuming package installed)
#install.packages("ggalluvial")
library(ggalluvial)
library(ggplot2)

# Define a color palette for the exporters
exporter_colors <- c(
  "Brazil"    = "#A8D0E6",  # soft sky blue
  "Vietnam"   = "#F6C1B9",  # pastel peach
  "Colombia"  = "#F6EAC2",  # light cream
  "Indonesia" = "#C1E1C1",  # soft mint green
  "Honduras"  = "#FFD6A5",  # pastel apricot
  "India"     = "#E2C0FF",  # lavender
  "Uganda"    = "#FFB7B2",  # soft pink
  "Ethiopia"  = "#B5EAEA"   # pale turquoise
)

# Title of graph will be Global Coffee Trade Flows (Top Exporters â†’ Key Importers, 2023)
ggplot(coffee,
       aes(axis1 = exporter, axis2 = importer, y = value_mkg)) +
  geom_alluvium(aes(fill = exporter), width = 1/12, alpha = 0.5, show.legend = FALSE) +
  geom_stratum(width = 1/12, fill = gray(0.99), color = gray(0.90)) +

  # Stratum labels
  geom_text(stat = "stratum", aes(
    label = after_stat(stratum),
    hjust = after_stat(ifelse(x == 1, 1, 0)),
    x = after_stat(ifelse(x == 1, x - 0.05, x + 0.05))  # reduced nudging
  ),
  size = 3.5, fontface = "bold", color = "black") +

  # Flow numbers (midpoints of alluvia)
  geom_text(aes(label = value_mkg), stat = "alluvium",
            size = 3, color = "black")+
  scale_fill_manual(values = exporter_colors) +
  scale_x_discrete(
    limits = c("Exporter", "Importer"),
    expand = expansion(mult = c(0.2, 0.2))
  ) +
  labs(
    title = "",
    y = "Million Kilograms of Coffee Beans",
    x = ""
  ) +
  theme_minimal(base_size = 12) +
  theme(
    axis.ticks = element_blank(),
    axis.text.y = element_blank(),
    panel.grid = element_blank(),
    plot.title = element_text(face = "bold", hjust = 0.5)
  )
```





# Air quality Events (API above moderate) in counties in  CONUS over the past few years
```{r}
#Data from EPA: https://aqs.epa.gov/aqsweb/airdata/download_files.html#AQI
# -------------------------------
# Load libraries (only once)
# -------------------------------
library(dplyr)
library(readr)
library(sf)
library(tigris)
library(ggplot2)
library(lubridate)
library(patchwork)

options(tigris_use_cache = TRUE)

# -------------------------------
# Load CONUS county geometries
# -------------------------------
counties <- counties(cb = TRUE, year = 2022) %>%
  st_as_sf() %>%
  filter(!STATEFP %in% c("02","15","60","66","69","72","78"))  # Remove AK, HI, territories

# Load CONUS state boundaries
states_sf <- states(cb = TRUE, year = 2022) %>%
  st_as_sf() %>%
  filter(!STATEFP %in% c("02","15","60","66","69","72","78"))

# -------------------------------
# Compute global maximum for color scale
# -------------------------------
all_years <- 2019:2024
max_unhealthy <- 0

for (yr in all_years) {
  df_tmp <- read_csv(paste0("daily_aqi_by_county_", yr, ".csv"))
  tmp_count <- df_tmp %>%
    filter(Category %in% c("Unhealthy", 
                           "Very Unhealthy", 
                           "Hazardous")) %>%
    group_by(`State Code`, `County Code`) %>%
    summarise(n = n(), .groups = "drop")
  max_unhealthy <- max(max_unhealthy, max(tmp_count$n, na.rm = TRUE))
}

# -------------------------------
# Function to plot AQI exceedances
# -------------------------------
plot_aqi_exceedances <- function(df, year) {
  
  exceedance_levels <- c("Unhealthy", 
                         "Very Unhealthy", 
                         "Hazardous")
  
  df_year <- df %>%
    filter(year(Date) == year)
  
  unhealthy_days <- df_year %>%
    filter(Category %in% exceedance_levels) %>%
    group_by(`State Code`, `County Code`, `State Name`, `county Name`) %>%
    summarise(unhealthy_days = n(), .groups = "drop") %>%
    mutate(FIPS = sprintf("%02s%03s", `State Code`, `County Code`))
  
  map_data <- counties %>%
    left_join(unhealthy_days, by = c("GEOID" = "FIPS"))
  
  ggplot() +
    geom_sf(data = map_data, aes(fill = unhealthy_days), color = NA) +
    geom_sf(data = states_sf, fill = NA, color = "black", size = 0.3) +
    scale_fill_gradient(
      low = "lightpink",
      high = "darkred",
      limits = c(0, max_unhealthy),
      na.value = "grey90"
    ) +
    labs(
      title = as.character(year),
      fill = "Number of Unhealthy Air Quality Days"
    ) +
    theme_minimal() +
    theme(
      plot.title = element_text(hjust = 0.5, size = 16),
      axis.title = element_blank(),
      axis.text = element_blank(),
      axis.ticks = element_blank()
    )
}

# -------------------------------
# Generate plots for each year
# -------------------------------
p2024 <- plot_aqi_exceedances(read_csv("daily_aqi_by_county_2024.csv"), 2024) & theme(legend.position = "none")
p2023 <- plot_aqi_exceedances(read_csv("daily_aqi_by_county_2023.csv"), 2023) & theme(legend.position = "none")
p2022 <- plot_aqi_exceedances(read_csv("daily_aqi_by_county_2022.csv"), 2022) & theme(legend.position = "none")
p2021 <- plot_aqi_exceedances(read_csv("daily_aqi_by_county_2021.csv"), 2021) & theme(legend.position = "none")
p2020 <- plot_aqi_exceedances(read_csv("daily_aqi_by_county_2020.csv"), 2020) & theme(legend.position = "none")
p2019 <- plot_aqi_exceedances(read_csv("daily_aqi_by_county_2019.csv"), 2019) & theme(legend.position = "none")

# -------------------------------
# Combine plots in 2x3 grid with one shared legend
# -------------------------------
combined <- (p2024 | p2023 | p2022) / 
            (p2021 | p2020 | p2019) + 
            plot_layout(guides = "collect") & theme(legend.position = "bottom")

# Display combined figure
combined

# Number of Wild fires according to national interagency fire center https://www.nifc.gov/fire-information/statistics/wildfires?utm_source=chatgpt.com

# 2019: 50,477
# 2020: 58,950
# 2021: 58,985
# 2022: 68,988
# 2023: 56,580
# 2024: 64,897

```




# Number and acres of wildfires each year in CA
```{r}
# DATA from: National Interagency Fire Center and CAL Fire
# Title: Acres of Wildfire Burn and Total Annual Precipitation in California from 2019-2024

library(ggplot2)

# Data
ca_fire_data <- data.frame(
  Year = 2019:2024,
  Precipitation = c(14.9, 5.8, 5.8, 28.4, 44.1, 25.2),
  Acres_Burned = c(4664364, 10122336, 7125643, 7577183, 2693910, 8924884)
)

# Scale factor for precipitation
max_acres <- max(ca_fire_data$Acres_Burned / 1e6)
max_precip <- max(ca_fire_data$Precipitation)
scale_factor <- max_acres / max_precip

# Plot
ggplot(ca_fire_data, aes(x = Year)) +
  
  # Bar for acres burned (in millions)
  geom_col(aes(y = Acres_Burned / 1e6), fill = "firebrick", alpha = 0.7) +
  
  # Line for precipitation, scaled
  geom_line(aes(y = Precipitation * scale_factor), color = "blue", size = 1.5) +
  geom_point(aes(y = Precipitation * scale_factor), color = "blue", size = 3) +
  
  # Y-axis labels
  scale_y_continuous(
    name = "Acres Burned (millions)",
    limits = c(0, 12),
    sec.axis = sec_axis(~ . / scale_factor, name = "Precipitation (inches)")
  ) +
  
  # X-axis breaks for each year
  scale_x_continuous(breaks = 2019:2024) +
  
  labs(
    title = "",
    x = "Year"
  ) +
  
  theme_minimal() +
  theme(
    plot.title = element_text(hjust = 0.5, size = 16),
    axis.title.y.left = element_text(color = "firebrick", size = 14),
    axis.title.y.right = element_text(color = "blue", size = 14),
    axis.text.x = element_text(size = 12),
    axis.text.y = element_text(size = 12)
  )


```



# Bad air quality and asthma hospitalizations in California across different median household income
```{r}
library(dplyr)
library(readr)
library(purrr)
library(stringr)

# Asthma data from California Department of public health https://data.chhs.ca.gov/dataset/asthma-hospitalization-rates-by-county
setwd("C:/Users/Alisha Yee Chan/Documents/Data/")
RawAsthmaData_County<-read.csv("asthma-hospitalization-rates-by-county-2015-Oct2024.csv")
AsthmaData_County_2015_2023<-filter(filter(RawAsthmaData_County, YEAR !=2024), COUNTY!="California")[,c(1,2,6)] # Only keep what's neccesary and Data in 2024 isn't complete so removing that year and removing California's total row
names(AsthmaData_County_2015_2023)[1]<-"County" # Rename column name COUNTY to lower case County
names(AsthmaData_County_2015_2023)[2]<-"Year"

# Air Quality events data from US EPA: https://aqs.epa.gov/aqsweb/airdata/download_files.html#AQI
# Directory where your CSV files are stored
path <- "C:/Users/Alisha Yee Chan/Documents/Data/"

# List all files matching the correct pattern
files <- list.files(path, pattern = "^annual_aqi_by_county_\\d{4}\\.csv$", full.names = TRUE)

# Load and combine all files into one dataframe
aqi_all <- files %>%
  map_dfr(~ read_csv(.x) %>%
            mutate(Year = str_extract(basename(.x), "\\d{4}")))

Cali_aqi_2015_2023<-data.frame(filter(aqi_all, State=="California"))
Cali_aqi_2015_2023$BadAQIDays<-Cali_aqi_2015_2023$Unhealthy.for.Sensitive.Groups.Days+Cali_aqi_2015_2023$Unhealthy.Days+Cali_aqi_2015_2023$Very.Unhealthy.Days+Cali_aqi_2015_2023$Hazardous.Days

# Income data
# Data from HDPulse: An Ecosystem of Minority Health and Health Disparities Resources. National Institute on Minority Health and Health Disparities. Created 9/16/2025. Available from https://hdpulse.nimhd.nih.gov


# California median household income and population count in 2023
ca_income_population_2023 <- data.frame(
  County = c("Trinity","Siskiyou","Imperial","Modoc",
             "Lake","Sierra","Humboldt","Tehama",
             "Lassen","Mendocino","Plumas","Merced",
             "Mariposa","Del Norte","Kern","Butte",
             "Kings","Tulare","Glenn","Fresno",
             "Shasta","Tuolumne","Inyo","Yuba",
             "Colusa","Sutter","Madera","Stanislaus",
             "Calaveras","Amador","San Bernardino","Nevada",
             "Mono","Los Angeles","San Joaquin","Sacramento",
             "Yolo","Riverside","San Luis Obispo","Monterey",
             "Santa Barbara","Solano","San Diego","Sonoma",
             "El Dorado","Ventura","San Benito","Napa",
             "Santa Cruz","Alpine","Orange","Placer",
             "Contra Costa","Alameda","San Francisco","Marin",
             "San Mateo","Santa Clara"),
  Median_Household_Income = c(53498,55499,56393,56648,58738,60000,61135,61834,
                              64395,64688,64946,65044,65378,66780,67660,68574,
                              68750,69489,70487,71434,71931,72259,72432,73313,
                              75149,75450,75496,79661,79877,81526,82184,84905,
                              86953,87760,88531,88724,88818,89672,93398,94486,
                              95977,99994,102285,102840,106190,107327,108289,108970,
                              109266,110781,113702,114678,125727,126240,141446,142785,
                              156000,159674),
  Population_count = c(13000,43000,181000,9000,67764,3000,135000,65000,
                       34000,87000,19000,300000,18000,28000,922529,228000,
                       152000,470000,29000,1024125,180000,55259,19000,80000,
                       22000,100000,165000,560000,46000,39000,2214281,100000,
                       15000,9663345,779233,1611231,230000,2529933,285000,435000,
                       448000,453000,3298799,510000,200000,835427,70000,140000,
                       275000,15000,3170435,400000,1172607,1649060,842000,270000,
                       774000,1877592)
)
library(dplyr)
library(ggplot2)
library(viridis)
library(scales)

# Step 1: Merge the asthma and AQI data
df <- AsthmaData_County_2015_2023 %>%
  inner_join(Cali_aqi_2015_2023, by = c("County", "Year")) %>%
  # Add Median Household Income + Population Count from 2023
  left_join(ca_income_population_2023, by = "County")

# Step 2: Fit Poisson model with population offset
poisson_model <- glm(NUMBER.OF.HOSPITALIZATIONS ~ BadAQIDays + Median_Household_Income +
                       offset(log(Population_count)),
                     data = df,
                     family = poisson())
summary(poisson_model)

# Step 3: Generate predictions for plotting
income_levels <- c(60000, 80000, 100000, 120000, 140000, 160000, 180000, 200000)

pred_df <- expand.grid(
  BadAQIDays = seq(min(df$BadAQIDays), max(df$BadAQIDays), length.out = 100),
  Median_Household_Income = income_levels
)

# Use the average population so estimates are in total hospitalizations
avg_pop <- mean(df$Population_count)
pred_df$Population_count <- avg_pop

# estimated total hospitalizations with standard errors
pred <- predict(poisson_model, newdata = pred_df, type = "link", se.fit = TRUE)

# Calculate 95% confidence intervals on response scale
pred_df$Predicted_Hosp <- exp(pred$fit)
pred_df$CI_lower <- exp(pred$fit - 1.96 * pred$se.fit)
pred_df$CI_upper <- exp(pred$fit + 1.96 * pred$se.fit)

# Keep Income_Label numeric for plotting, ordered high â†’ low
pred_df$Income_Label <- factor(pred_df$Median_Household_Income,
                               levels = rev(income_levels))

# Create formatted labels for legend
income_labels <- setNames(dollar(income_levels), income_levels)

# Step 4: Plot estimated total hospitalizations with shaded confidence intervals
ggplot(pred_df, aes(x = BadAQIDays, y = Predicted_Hosp, color = Income_Label, fill = Income_Label)) +
  geom_line(size = 1.2) +
  geom_ribbon(aes(ymin = CI_lower, ymax = CI_upper), alpha = 0.2, color = NA) +
  labs(x = "Number of Unhealthy Air Quality Days in California",
       y = "Estimated Number of Asthma Hospitalizations in California",
       color = "Median Household Income",
       fill = "Median Household Income") +
  scale_color_viridis_d(option = "F", labels = income_labels) +
  scale_fill_viridis_d(option = "F", labels = income_labels) +
  theme_minimal()


```












# Population growth rate
```{r}
# Population data from world development indicators. 1960-2024
# Long-Term Acceleration or Deceleration of Population Growth by Country (1960-2024)
# ------------------------------
# 1. Read your CSV file
# ------------------------------
setwd("C:/Users/Alisha Yee Chan/Documents/Data/")
WorldCountryPopoulationRaw <- read.csv("WorldCountryPopulationsCount.csv", stringsAsFactors = FALSE)

# Remove unnecessary columns
WorldCountryPopoulationRaw$Country.Name <- NULL
WorldCountryPopoulationRaw$Indicator.Name <- NULL
WorldCountryPopoulationRaw$Indicator.Code <- NULL

# ------------------------------
# 2. Load required libraries
# ------------------------------
library(dplyr)
library(tidyr)
library(stringr)
library(ggplot2)
library(sf)
library(rnaturalearth)
library(rnaturalearthdata)

# ------------------------------
# 3. Reshape from wide to long
# ------------------------------
WorldCountryPopoulation <- WorldCountryPopoulationRaw %>%
  pivot_longer(
    cols = starts_with("X"),       
    names_to = "Year",
    values_to = "Population"
  ) %>%
  mutate(
    Year = str_remove(Year, "^X"),  
    Year = as.integer(Year)         
  ) %>%
  arrange(Country.Code, Year)

# ------------------------------
# 4. Calculate annual growth rate (year-over-year %)
# ------------------------------
WorldCountryPopoulation <- WorldCountryPopoulation %>%
  group_by(Country.Code) %>%
  mutate(
    growth_rate = (Population - lag(Population)) / lag(Population) * 100
  )

# ------------------------------
# 5. Compute long-term slope of growth rate for each country
# ------------------------------
country_trend <- WorldCountryPopoulation %>%
  group_by(Country.Code) %>%
  summarize(
    slope = ifelse(sum(!is.na(growth_rate)) > 1,
                   coef(lm(growth_rate ~ Year, data = cur_data()))[2],
                   NA_real_)
  )

# ------------------------------
# 6. Load world map
# ------------------------------
world <- ne_countries(scale = "medium", returnclass = "sf")

# ------------------------------
# 7. Join map with slope data using Country.Code
# ------------------------------
world_trend <- world %>%
  left_join(country_trend, by = c("iso_a3" = "Country.Code"))

# ------------------------------
# 8. Plot the map with gradient
# ------------------------------
ggplot(world_trend) +
  geom_sf(aes(fill = slope)) +
  scale_fill_gradient2(
    low = "darkorange", mid = "white", high = "purple", midpoint = 0,
    name = "Change in Growth Rate\n(% per yearÂ²)",
    guide = guide_colorbar(
      barwidth = 15,       # wider color bar
      barheight = 1.5,     # taller bar
      title.position = "top",  # title above the bar
      title.hjust = 0.5    # center the title
    )
  ) +
  theme_minimal() +
  labs(
    title = "",
    subtitle = "Orange = slowing growth, Purple = accelerating growth, White = steady"
  ) +
  theme(
    legend.position = "bottom",
    legend.title.align = 0.5,  # center legend title
    legend.text.align = 0.5,   # center legend labels
    plot.subtitle = element_text(hjust = 0.5)  # center subtitle
  )

```


# Human Rabies Deaths by country at different country GDP
```{r}
#Title:Countries with the Highest Human Rabies Deaths Over the Last 5 Years (2019â€“2024)

# Rabies data from the world health organization: https://www.who.int/data/gho/data/indicators/indicator-details/GHO/reported-number-of-human-rabies-deaths?utm_source=chatgpt.com
# 2019-2024

setwd("C:/Users/Alisha Yee Chan/Documents/Data/")
HumanRabiesCountry2024_2010<-read.csv("RabiesCountry2024_2010.csv")
HumanRabiesCountry2024_2010$Rabies.Cases<-as.numeric(HumanRabiesCountry2024_2010$Rabies.Cases)

# I only want the past 5 years
HumanRabiesCountry2019_2024 <- HumanRabiesCountry2024_2010 %>%
  filter(Period >= 2019 & Period <= 2024)

library(dplyr)

# Aggregate sum of rabies deaths for each country
Sum_agg_RabiesCountry <- HumanRabiesCountry2019_2024 %>%
  group_by(Location) %>%
  summarise(total_rabies_deaths = sum(Rabies.Cases, na.rm = TRUE)) %>%
  arrange(desc(total_rabies_deaths))

# Population data from world development indicators
setwd("C:/Users/Alisha Yee Chan/Documents/Data/")
WorldCountryPopulationRaw<-read.csv("WorldCountryPopulationsCount.csv")
WorldCountryPopulation<-WorldCountryPopulationRaw[,c(1,2,69)]
names(WorldCountryPopulation)<-c("Location","Country.Code","Population")

# # Full join by country
# merged <- full_join(WorldCountryPopulation, Sum_agg_RabiesCountry, by = "Location")
# 
# # Only in WorldCountryPopulation, not in Sum_agg_RabiesCountry
# only_in_population <- merged %>%
#   filter(is.na(total_rabies_deaths))
# 
# # Only in Sum_agg_RabiesCountry, not in WorldCountryPopulation
# only_in_rabies <- merged %>%
#   filter(is.na(Population))
# 
# only_in_population
# only_in_rabies


# ------------------------------
# 2. Standardize country names in Rabies dataset
# ------------------------------

# Mapping Rabies dataset names â†’ Population dataset names
name_mapping <- c(
  "United States of America" = "United States",
  "United Republic of Tanzania" = "Tanzania",
  "Iran (Islamic Republic of)" = "Iran, Islamic Rep.",
  "Bolivia (Plurinational State of)" = "Bolivia",
  "TÃ¼rkiye" = "Turkiye",
  "Venezuela (Bolivarian Republic of)" = "Venezuela, RB",
  "Egypt" = "Egypt, Arab Rep.",
  "Lao People's Democratic Republic" = "Lao PDR",
  "Kyrgyzstan" = "Kyrgyz Republic",
  "Republic of Moldova" = "Moldova",
  "United Kingdom of Great Britain and Northern Ireland" = "United Kingdom",
  "Netherlands (Kingdom of the)" = "Netherlands",
  "Bahamas" = "Bahamas, The",
  "Congo" = "Congo, Rep.",
  "Cook Islands" = "Cook Islands",
  "Democratic People's Republic of Korea" = "Korea, Dem. People's Rep.",
  "Democratic Republic of the Congo" = "Congo, Dem. Rep.",
  "Gambia" = "Gambia, The",
  "Micronesia (Federated States of)" = "Micronesia, Fed. Sts.",
  "Niue" = "Niue",
  "Republic of Korea" = "Korea, Rep.",
  "Saint Kitts and Nevis" = "St. Kitts and Nevis",
  "Saint Lucia" = "St. Lucia",
  "Saint Vincent and the Grenadines" = "St. Vincent and the Grenadines",
  "Slovakia" = "Slovak Republic",
  "Yemen" = "Yemen, Rep."  # <-- added this line
)


# Rename countries in Rabies dataset
Sum_agg_RabiesCountry <- Sum_agg_RabiesCountry %>%
  mutate(Location = ifelse(Location %in% names(name_mapping),
                           name_mapping[Location],
                           Location))

# ------------------------------
# 3. Full join datasets
# ------------------------------

full_merged <- full_join(WorldCountryPopulation, Sum_agg_RabiesCountry,
                         by = "Location")

# ------------------------------
# 4. Identify unmatched countries
# ------------------------------

unmatched <- full_merged %>%
  mutate(
    missing_from = case_when(
      is.na(Population) & !is.na(total_rabies_deaths) ~ "Missing from Population",
      !is.na(Population) & is.na(total_rabies_deaths) ~ "Missing from Rabies",
      TRUE ~ NA_character_
    )
  ) %>%
  filter(!is.na(missing_from)) %>%
  select(Location, Population, total_rabies_deaths, missing_from)

# Minimal unmatched now

Combined_dataset<-merge(Sum_agg_RabiesCountry, WorldCountryPopulation, by="Location")

Combined_dataset_TopCountries<-filter(Combined_dataset, total_rabies_deaths>50) 

Combined_dataset_TopCountries$Location <- factor(Combined_dataset_TopCountries$Location, levels = Combined_dataset_TopCountries$Location[order(Combined_dataset_TopCountries$total_rabies_deaths)]) # put data in decending order

library(ggplot2)
library(scales)

ggplot(Combined_dataset_TopCountries, aes(x = Location, y = total_rabies_deaths, fill = total_rabies_deaths)) +
  geom_bar(stat = "identity") +
  geom_text(aes(label = scales::comma(total_rabies_deaths)), 
            hjust = -0.1, size = 3.5, color = "black") +
  coord_flip() +
  labs(title = NULL, x = "Country", y = "Human Deaths from Rabies") +
  theme_minimal(base_size = 14) +
  theme(
    axis.title = element_text(face = "bold"),
    axis.text.y = element_text(size = 12),
    axis.text.x = element_text(size = 10),
    plot.margin = margin(10, 20, 10, 10),
    legend.position = "none"
  ) +
  scale_y_continuous(labels = scales::comma, expand = expansion(mult = c(0, 0.1))) +
  scale_fill_gradient(low = "#FAF0E6", high = "#8B0000")  # light beige to dark red


```


# STM of presidential addresses topics
```{r}
# Inaugural Addresses: Topic Proportions per Speech
# Install required packages if not already installed
#install.packages("stm")
#install.packages("tidyverse")
#install.packages("tm")
# ===============================
# Inaugural Addresses STM Script
# ===============================

# Load libraries
library(stm)
library(tidyverse)
library(tm)
#install.packages("SnowballC")
library(SnowballC)

# -------------------------------
# 1. Load speeches
# -------------------------------
speech_folder <- "C:/Users/Alisha Yee Chan/Documents/Data/InagurationSpeaches"
speech_files <- list.files(speech_folder, pattern = "*.txt", full.names = TRUE)

speech_texts <- lapply(speech_files, function(f) {
  text <- readLines(f, warn = FALSE)
  paste(text, collapse = " ")
})

names(speech_texts) <- basename(speech_files)

speeches_df <- data.frame(
  doc_id = names(speech_texts),
  text = unlist(speech_texts),
  stringsAsFactors = FALSE
)

# -------------------------------
# 2. Create corpus & find very frequent words
# -------------------------------
corpus <- Corpus(VectorSource(speeches_df$text))
corpus <- tm_map(corpus, content_transformer(tolower))
corpus <- tm_map(corpus, content_transformer(removeNumbers))
corpus <- tm_map(corpus, content_transformer(removePunctuation))
corpus <- tm_map(corpus, stripWhitespace)

dtm <- DocumentTermMatrix(corpus)
library(slam)
term_doc_freq <- col_sums(dtm > 0) / nDocs(dtm)

# Remove words appearing in >75% of speeches
very_common_words <- names(term_doc_freq[term_doc_freq > 0.75])
print("Very frequent words removed:")
print(very_common_words)

# -------------------------------
# 3. Define custom stopwords
# -------------------------------
custom_stopwords <- c(
  stopwords("english"),
  very_common_words,
  "govern", "government", "governing",
  "law", "president", "presidential"
)

# Stem stopwords
custom_stopwords_stemmed <- wordStem(custom_stopwords, language = "english")

# -------------------------------
# 4. Preprocess for STM
# -------------------------------
processed <- textProcessor(
  documents = speeches_df$text,
  metadata = speeches_df,
  lowercase = TRUE,
  removestopwords = TRUE,
  removenumbers = TRUE,
  stem = TRUE,
  customstopwords = custom_stopwords_stemmed
)

out <- prepDocuments(processed$documents, processed$vocab, processed$meta)
docs <- out$documents
vocab <- out$vocab
meta <- out$meta

# -------------------------------
# 5. Fit STM model
# -------------------------------
K <- 4  # number of topics
stm_model <- stm(docs, vocab, K = K, max.em.its = 75, data = meta, init.type = "LDA")

# Inspect Topics
labelTopics(stm_model, n = 10)

# -------------------------------
# 6. 2x2 Thematic Map
# -------------------------------
theta_mat <- stm_model$theta

# Axis mapping based on your corrected topics:
# Topic 1: Civic Duty / National Security
# Topic 2: Civil Rights / Peace
# Topic 3: War / Defense
# Topic 4: Global Peace / Freedom

X_score <- theta_mat[,3] - (theta_mat[,1] + theta_mat[,2] + theta_mat[,4])  # War vs Domestic/Peace
Y_score <- theta_mat[,1] + theta_mat[,3] - (theta_mat[,2] + theta_mat[,4])  # Governance/Civic Duty vs Civil Rights/Peace

plot_df <- data.frame(
  doc_id = meta$doc_id,
  X = X_score,
  Y = Y_score
)

library(ggplot2)
ggplot(plot_df, aes(x = X, y = Y, label = doc_id)) +
  geom_point(color = "blue", size = 3) +
  geom_text(vjust = -0.5, hjust = 0.5, size = 3) +
  geom_hline(yintercept = 0, linetype = "dashed") +
  geom_vline(xintercept = 0, linetype = "dashed") +
  xlab("Domestic/Peace â†” War/Foreign") +
  ylab("Civic Duty / Security â†” Civil Rights / Peace") +
  ggtitle("Inaugural Addresses: 2x2 Thematic Map") +
  theme_minimal()

library(ggplot2)
library(reshape2)
library(tools)  # for file_path_sans_ext

# 1. Clean doc names (remove path and .txt)
doc_names <- basename(speech_files)
doc_names_clean <- file_path_sans_ext(doc_names)

# 2. Get file modification times and order
file_info <- file.info(speech_files)
file_order <- order(file_info$mtime)  # indices of files sorted by last modified

# 3. Reorder theta matrix and meta$doc_id to match this order
theta_df <- as.data.frame(theta_mat)
theta_df$doc_id <- doc_names_clean

# Apply the order
theta_df <- theta_df[file_order, ]

# Convert doc_id to factor so ggplot respects order
theta_df$doc_id <- factor(theta_df$doc_id, levels = theta_df$doc_id)

# 4. Melt for ggplot
theta_long <- melt(theta_df, id.vars = "doc_id", variable.name = "Topic", value.name = "Proportion")

# Map Topic column to descriptive names (factor)
theta_long$Topic <- factor(theta_long$Topic,
                           levels = c("V1", "V2", "V3", "V4"),
                           labels = c("Constitutional Rights", 
                                      "National Unity", 
                                      "Civic Duty", 
                                      "Peace"))

# Define colors for each topic by label
topic_colors <- c(
  "Constitutional Rights" = "#BFD8B8",       # soft mint green
  "National Unity" = "#FFD6A5",     # light peach
  "Civic Duty" = "#9EC1CF",    # muted sky blue
  "Peace" = "#F7D6E0" # gentle pink
)

# Plot with correct legend and colors
ggplot(theta_long, aes(x = doc_id, y = Proportion, fill = Topic)) +
  geom_bar(stat = "identity") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 90, hjust = 1, vjust=0.25),
            legend.position = "bottom") +
  ylab("Topic Proportion") +
  xlab("Speech") +
  ggtitle("") +
  scale_fill_manual(values = topic_colors, name = "Topic")


labelTopics(stm_model, n = 10)

```

# Number of government shutdowns
```{r}
library(ggplot2)
library(dplyr)
library(ggfx)

# Dataset
shutdowns <- data.frame(
  Year = c("1980", "1981", "1984", "1986", "1990", "1995 (Nov)", "1995â€“1996", 
           "2013", "2018 (Jan)", "2018â€“19", "2025 (?)"),
  Days = c(1, 1, 0.17, 0.17, 3, 5, 21, 16, 3, 35, 0),
  President = c("Jimmy Carter", "Ronald Reagan", "Ronald Reagan", "Ronald Reagan", 
                "George H. W. Bush", "Bill Clinton", "Bill Clinton", 
                "Barack Obama", "Donald Trump", "Donald Trump", "Donald Trump")
)

shutdowns <- shutdowns %>%
  mutate(
    Label = paste(Year, President, sep = "\n"),
    # Make Label a factor to preserve order
    Label = factor(Label, levels = Label)
  )

# Plot
ggplot(shutdowns, aes(x = Label, y = Days)) +
  with_shadow(
    geom_bar(stat = "identity", fill = "skyblue", width = 0.7),
    x_offset = 1, y_offset = 1, sigma = 2, colour = "grey70", alpha = 0.3
  ) +
  # Add question mark only above the last bar
  geom_text(
    data = shutdowns %>% filter(Year == "2025 (?)"),
    aes(label = "?"),
    vjust = -0.5,
    color = "#333333",
    size = 6
  ) +
  theme_minimal(base_size = 14) +
  theme(
    plot.background = element_rect(fill = "#f5f5f5"),
    panel.background = element_rect(fill = "#f5f5f5"),
    panel.grid.major = element_line(color = "lightgray"),
    panel.grid.minor = element_blank(),
    axis.text.x = element_text(angle = 45, hjust = 1, face = "bold", color = "#333333"),
    axis.text.y = element_text(color = "#333333"),
    axis.title.y = element_text(color = "#333333"),
    plot.title = element_text(color = "#333333", face = "bold", hjust = 0.5),
    legend.position = "none"
  ) +
  ylab("Shutdown Duration (Days)") +
  xlab("") +
  scale_y_continuous(breaks = seq(0, 40, by = 5), expand = c(0,0))


```